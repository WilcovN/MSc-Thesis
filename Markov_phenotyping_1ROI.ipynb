{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov phenotyping 1 ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use cellpose env\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to load the csv data into tensor format\n",
    "This code dynamically loads the csv files in the same folder \n",
    "1. It creates a 3d tensor where each 2d matrix is a csv file\n",
    "2. Saves the name of the file as its feature name in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 547, 101])\n",
      "['tb_brightness', 'tb_coordinates_x', 'tb_circularity', 'tb_coordinates_y', 'tb_cell_category', 'tb_area']\n",
      "There are suppose to be 6 feature dimensions in the torch tensor\n"
     ]
    }
   ],
   "source": [
    "path_to_experiment = r\"\\\\store\\department\\gene\\chien_data\\Lab\\Data_and_Analysis\\Wilco_van_Nes\\WvN014_phenotype_imaging_strategy5_20250217\\_from_Li_Results_ROI9\"\n",
    "\n",
    "if not os.path.exists(path_to_experiment):\n",
    "    print(\"path to experiment folder doesn't exist\")\n",
    "\n",
    "path_to_table_data = os.path.join(path_to_experiment, \"template_feature_tables\")\n",
    "\n",
    "if not os.path.exists(path_to_experiment):\n",
    "    print(\"path to table folder doesn't exist\")\n",
    "\n",
    "csv_files_paths = glob.glob(os.path.join(path_to_table_data, \"*.csv\"))\n",
    "\n",
    "tensor_list = []\n",
    "feature_name_list = []\n",
    "\n",
    "for csv_file_path in csv_files_paths:\n",
    "\n",
    "    feature_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "    feature_name_list.append(feature_name)\n",
    "    # print(feature_name_list)\n",
    "\n",
    "    with open(csv_file_path, newline = '') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        data = [list(map(float, row)) for row in reader]\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "        if data_tensor.shape[1] == 1:\n",
    "            data_tensor = data_tensor.repeat(1, 101)\n",
    "\n",
    "        tensor_list.append(data_tensor)\n",
    "        # print(data_tensor.shape)\n",
    "        # print(len(tensor_list))\n",
    "\n",
    "all_cells_tensor = torch.stack(tensor_list, dim=0)\n",
    "print(all_cells_tensor.shape)\n",
    "print(feature_name_list)\n",
    "print(f\"There are suppose to be {len(feature_name_list)} feature dimensions in the torch tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov transition matrix estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create boxes inside the image were you calculate the state (counting the number of individual T cells and cancer cells centroids in the region)\n",
    "We also figure out from this how many unique states there are for the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimensions are valid: 2000x1800 is divisible by box size 100\n",
      "All cells were put in boxes correctly, placed 547.0 cells need to place 547 cells for the first frame\n",
      "Number of unique states: 77\n",
      "Unique states: {(4, 0), (5, 1), (8, 0), (0, 5), (2, 2), (6, 2), (7, 1), (4, 2), (3, 6), (5, 3), (8, 2), (0, 7), (2, 4), (1, 8), (6, 4), (3, 8), (5, 5), (8, 4), (11, 0), (0, 0), (0, 9), (1, 10), (6, 6), (3, 1), (0, 2), (1, 3), (3, 3), (5, 0), (1, 5), (6, 1), (7, 0), (3, 5), (5, 2), (4, 4), (9, 0), (1, 7), (2, 6), (3, 7), (5, 4), (4, 6), (9, 2), (1, 0), (0, 11), (6, 8), (3, 0), (4, 8), (1, 2), (0, 4), (2, 1), (0, 13), (2, 10), (3, 2), (4, 1), (8, 1), (1, 4), (0, 6), (2, 3), (0, 15), (6, 3), (3, 4), (4, 3), (10, 0), (1, 6), (0, 8), (2, 5), (6, 5), (4, 5), (8, 5), (9, 4), (0, 1), (11, 1), (0, 10), (1, 1), (0, 3), (2, 0), (2, 9), (6, 0)}\n"
     ]
    }
   ],
   "source": [
    "# box pixel size\n",
    "box_size = 100\n",
    "\n",
    "# image pixel size\n",
    "# image_width = 5120\n",
    "# image_height = 5120\n",
    "image_width = 2000\n",
    "image_height = 1800\n",
    "\n",
    "# Integer check to ensure image dimensions are divisible by box size\n",
    "if image_width % box_size != 0 or image_height % box_size != 0:\n",
    "    raise ValueError(f\"Image dimensions ({image_width}x{image_height}) must be divisible by box size ({box_size})\")\n",
    "else:\n",
    "    print(f\"Image dimensions are valid: {image_width}x{image_height} is divisible by box size {box_size}\")\n",
    "\n",
    "# The tensor to store the states of each box\n",
    "# 1st dimension: frame number\n",
    "# 2nd dimension: x position of the box in the image\n",
    "# 3rd dimension: y position of the box in the image\n",
    "# 4th dimension: state values (e.g., T-cells and cancer cells) (τ, κ)\n",
    "\n",
    "state_tensor = torch.zeros(all_cells_tensor.shape[2],\n",
    "                           image_width // box_size, \n",
    "                           image_height // box_size, \n",
    "                           2)\n",
    "\n",
    "unique_states = set()\n",
    "\n",
    "# Function to calculate the box index for each cell\n",
    "def get_box_index(x, y, box_size, image_width, image_height):\n",
    "\n",
    "    # Calculate box index along the x-axis and y-axis\n",
    "    box_x_index = int(min(x // box_size, (image_width // box_size) - 1))\n",
    "    box_y_index = int(min(y // box_size, (image_height // box_size) - 1))\n",
    "\n",
    "    return box_x_index, box_y_index\n",
    "\n",
    "# for loop to calculate the state\n",
    "for frame in range(all_cells_tensor.shape[2]):\n",
    "    #first get the position of the feature in the all_cells_tensor\n",
    "    x_tensor_pos = feature_name_list.index(\"tb_coordinates_x\")\n",
    "    y_tensor_pos = feature_name_list.index(\"tb_coordinates_y\")\n",
    "    cell_type_tensor_pos = feature_name_list.index(\"tb_cell_category\")\n",
    "\n",
    "    x_coord_frame = all_cells_tensor[x_tensor_pos, :, frame]\n",
    "    y_coord_frame = all_cells_tensor[y_tensor_pos, :, frame]\n",
    "    cell_type_frame = all_cells_tensor[cell_type_tensor_pos, :, frame]\n",
    "\n",
    "    # Find the position of the cell in which box and adjust the state of the box\n",
    "    for cell_id, (x, y, cell_type) in enumerate(zip(x_coord_frame, y_coord_frame, cell_type_frame)):\n",
    "        # print(f\"x is:{x}\")\n",
    "        # print(f\"y is:{y}\")\n",
    "\n",
    "        box_x_index, box_y_index = get_box_index(x, y, box_size, image_width, image_height)\n",
    "\n",
    "        # print(f\"box x index is {box_x_index} and type {type(box_x_index)}, box y index is {box_y_index}\")\n",
    "\n",
    "        if cell_type == 2:\n",
    "            state_tensor[frame, box_x_index, box_y_index, 0] += 1\n",
    "        if cell_type == 0 or cell_type == 1:\n",
    "            state_tensor[frame, box_x_index, box_y_index, 1] += 1\n",
    "\n",
    "    # Cell check\n",
    "    if torch.sum(state_tensor[frame]) != all_cells_tensor.shape[1]:\n",
    "        print(f\"Not all cells were put in boxes correctly placed {torch.sum(state_tensor[frame])} need to place {all_cells_tensor.shape[1]} cells\")\n",
    "    elif frame == 0:\n",
    "        print(f\"All cells were put in boxes correctly, placed {torch.sum(state_tensor[frame])} cells need to place {all_cells_tensor.shape[1]} cells for the first frame\")\n",
    "    \n",
    "    # Iterate over the boxes to find the unique states\n",
    "    for box_x in range(image_width // box_size):\n",
    "        for box_y in range(image_height // box_size):\n",
    "            state_t = state_tensor[frame, box_x, box_y]\n",
    "            t_cells = int(state_t[0])\n",
    "            cancer_cells = int(state_t[1])\n",
    "            unique_states.add((t_cells, cancer_cells))\n",
    "\n",
    "# Give each state a index for the transition matrix\n",
    "state_to_index = {tuple(state): i for i, state in enumerate(unique_states)}\n",
    "print(f\"Number of unique states: {len(unique_states)}\")\n",
    "print(f\"Unique states: {unique_states}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we create the markov transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detachment Probability (pd): 0.0064344825223088264\n"
     ]
    }
   ],
   "source": [
    "# The markov transition tnsor stores the states of each box\n",
    "# 1st dimension: frame number\n",
    "# 2nd dimension: x position of the box in the image\n",
    "# 3rd dimension: y position of the box in the image\n",
    "# 4th dimension: state values (e.g., T-cells and cancer cells) (τ, κ) at t\n",
    "# 5th dimension: state values (e.g., T-cells and cancer cells) (τ, κ) at t+1\n",
    "\n",
    "markov_transition_tensor = torch.zeros(all_cells_tensor.shape[2]-1,\n",
    "                                image_width // box_size,\n",
    "                                image_height // box_size,\n",
    "                                len(unique_states),\n",
    "                                len(unique_states))\n",
    "\n",
    "# For loop to fill in the transition matrix with all the states\n",
    "for frame in range(all_cells_tensor.shape[2]-1):\n",
    "    for box_x in range(image_width // box_size):\n",
    "        for box_y in range(image_height // box_size):\n",
    "            state_t = tuple(state_tensor[frame, box_x, box_y].tolist())\n",
    "            state_t_plus_1 = tuple(state_tensor[frame + 1, box_x, box_y].tolist())\n",
    "\n",
    "            state_t_index = state_to_index[state_t]\n",
    "            state_t_plus_1_index = state_to_index[state_t_plus_1]\n",
    "\n",
    "            if frame ==0:\n",
    "                markov_transition_tensor[frame, box_x, box_y, state_t_index, state_t_plus_1_index] += 1\n",
    "            else:\n",
    "                markov_transition_tensor[frame, box_x, box_y] = markov_transition_tensor[frame-1, box_x, box_y].clone()\n",
    "                markov_transition_tensor[frame, box_x, box_y, state_t_index, state_t_plus_1_index] += 1\n",
    "\n",
    "# Create a seperate normalized matrix to seperate for counts\n",
    "normalized_markov_transition_tensor = markov_transition_tensor.clone().float()\n",
    "\n",
    "# For loop to normalize the transition matrix\n",
    "for frame in range(all_cells_tensor.shape[2]-1):\n",
    "    for box_x in range(image_width // box_size):\n",
    "        for box_y in range(image_height // box_size):\n",
    "            row_sums = normalized_markov_transition_tensor[frame, box_x, box_y].sum(dim=1)\n",
    "\n",
    "            # Avoid division by 0, 0/1=0 so will still create proper matrix\n",
    "            row_sums[row_sums == 0] = 1\n",
    "\n",
    "            normalized_markov_transition_tensor[frame, box_x, box_y] = normalized_markov_transition_tensor[frame, box_x, box_y]/row_sums.unsqueeze(1)\n",
    "\n",
    "total_possible_transitions = (image_width // box_size) * (image_height // box_size) * (all_cells_tensor.shape[2] - 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detachment Probability (pd): 0.0064344825223088264\n"
     ]
    }
   ],
   "source": [
    "# For loop to calculate the detatchment probability\n",
    "start_state_detachment = (1,1)\n",
    "start_state_detachment_index = state_to_index[start_state_detachment]\n",
    "detachment_probability = 0\n",
    "\n",
    "for frame in range(all_cells_tensor.shape[2]-1):\n",
    "    for box_x in range(image_width // box_size):\n",
    "        for box_y in range(image_height // box_size):\n",
    "\n",
    "            state_t_plus_1 = tuple(state_tensor[frame + 1, box_x, box_y].tolist())\n",
    "            state_t_plus_1_index = state_to_index[state_t_plus_1]\n",
    "\n",
    "            if state_t_plus_1[0] == 0:\n",
    "                detachment_probability += markov_transition_tensor[frame, box_x, box_y, start_state_detachment_index, state_t_plus_1_index]\n",
    "\n",
    "detachment_probability = detachment_probability/total_possible_transitions\n",
    "print(f\"Detachment Probability (pd): {detachment_probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of T cells over time\n",
    "\n",
    "# number of cancer cells over time \n",
    "\n",
    "# mean cancer cell area\n",
    "\n",
    "# mean T-cell area\n",
    "\n",
    "# mean cancer cell roundness\n",
    "\n",
    "# mean T-cell roundness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deprecated/test code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to load the data as a panda's datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 547 entries, cell_1 to cell_547\n",
      "Columns: 506 entries, ('tb_brightness', 'frame_0') to ('tb_area', 'frame_100')\n",
      "dtypes: float64(204), int64(302)\n",
      "memory usage: 2.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "path_to_experiment = r\"\\\\store\\department\\gene\\chien_data\\Lab\\Data_and_Analysis\\Wilco van Nes\\WvN014_phenotype_imaging_strategy5_20250217\\_from_Li_Results_ROI9\"\n",
    "\n",
    "if not os.path.exists(path_to_experiment):\n",
    "    print(\"path to experiment folder doesn't exist\")\n",
    "\n",
    "path_to_table_data = os.path.join(path_to_experiment, \"template_feature_tables\")\n",
    "if not os.path.exists(path_to_experiment):\n",
    "    print(\"path to table folder doesn't exist\")\n",
    "\n",
    "\n",
    "csv_files_paths = glob.glob(os.path.join(path_to_table_data, \"*.csv\"))\n",
    "\n",
    "dfs =[]\n",
    "feature_name_list = []\n",
    "\n",
    "for csv_file_path in csv_files_paths:\n",
    "    df = pd.read_csv(csv_file_path, header=None)\n",
    "    feature_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "    feature_name_list.append(feature_name)\n",
    "\n",
    "    # print(feature_name)\n",
    "    # print(df.shape)\n",
    "\n",
    "    frames = [f'frame_{i}' for i in range(df.shape[1])]\n",
    "    df.columns = frames\n",
    "    dfs.append(df)\n",
    "\n",
    "all_cells_df = pd.concat(dfs, axis=1, keys=feature_name_list)\n",
    "\n",
    "all_cells_df.index = [f'cell_{i+1}' for i in range(all_cells_df.shape[0])]\n",
    "\n",
    "print(all_cells_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tb_cell_category tb_brightness                                      \\\n",
      "                frame_0       frame_0  frame_1  frame_2  frame_3  frame_4   \n",
      "cell_1                0        98.601   98.059   99.898  102.370   97.427   \n",
      "cell_2                0        98.601   98.059   99.898  102.370   97.427   \n",
      "cell_3                2        89.867   92.742   97.872   90.343   87.914   \n",
      "cell_4                2        97.040  103.230  104.270  103.820  101.600   \n",
      "cell_5                0       101.120  102.340  103.640  102.860  100.240   \n",
      "\n",
      "                                           ...  tb_area                    \\\n",
      "        frame_5  frame_6 frame_7  frame_8  ... frame_91 frame_92 frame_93   \n",
      "cell_1  100.280   98.531  97.695   98.941  ...      548     7004     8241   \n",
      "cell_2  100.280   98.531  97.695   98.941  ...      548     7004     8241   \n",
      "cell_3   91.324   98.531  88.954   88.955  ...     1896     1989     1873   \n",
      "cell_4  102.760  101.590  99.716   99.608  ...     1389     1347     1471   \n",
      "cell_5   97.904   99.882  96.898  100.310  ...     7291     2230     1206   \n",
      "\n",
      "                                                                        \n",
      "       frame_94 frame_95 frame_96 frame_97 frame_98 frame_99 frame_100  \n",
      "cell_1     9655     9456     3088     3520     3740     1380      1355  \n",
      "cell_2     9655     9456     3088     3520     3740     1380      1355  \n",
      "cell_3     1769     1687     1514     1168      668      646       621  \n",
      "cell_4     1366     1316     1381     1422     1487     1530      1439  \n",
      "cell_5     5157     1317     3612     3040     2486     2647      2805  \n",
      "\n",
      "[5 rows x 507 columns]\n"
     ]
    }
   ],
   "source": [
    "cols_to_print = ['tb_cell_category'] + [f'{feature}' for feature in feature_name_list]\n",
    "print(all_cells_df[cols_to_print].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([547, 506])\n"
     ]
    }
   ],
   "source": [
    "all_cells_tensor = torch.tensor(all_cells_df.values, dtype=torch.float32)\n",
    "print(all_cells_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own values/definitions\n",
    "101 time points, timepoint = 3 minutes interval\n",
    "ROI9 crop FOV of (2000,1800) square\n",
    "\n",
    "individual cell features\n",
    "* centroid position\n",
    "* morphology\n",
    "    * area\n",
    "    * circularity\n",
    "* speed\n",
    "* directionality\n",
    "\n",
    "Dynamic 3d feature tensor\n",
    "\n",
    "\n",
    "\n",
    "<!-- ```python\n",
    "cell_data = {\n",
    "    cell_id: {\n",
    "        \"frames\": [t1, t2, t3, ...],      # List of frame timepoints when the cell appears\n",
    "        \"x\": [x1, x2, x3, ...],           # X coordinates over time\n",
    "        \"y\": [y1, y2, y3, ...],           # Y coordinates over time\n",
    "        \"area\": [a1, a2, a3, ...],        # Cell area over time\n",
    "        \"brightness\": [b1, b2, b3, ...],  # Brightness over time\n",
    "        \"circularity\": [c1, c2, c3, ...], # Circularity over time\n",
    "        \"type\": cell_type,                # 0 = non-MN tumor, 1 = MN tumor, 2 = immune cell\n",
    "        \"parent\": parent_id,              # If cell was born from division, store parent ID\n",
    "    }\n",
    "}\n",
    "``` -->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper values/definitions\n",
    "Verma, A., Yu, C., Bachl, S., Lopez, I., Schwartz, M., Moen, E., ... & Engelhardt, B. E. (2024). Cellular behavior analysis from live-cell imaging of TCR T cell–cancer cell interactions. bioRxiv.\n",
    "\n",
    "#### Algorithm details\n",
    "Neighborhood encoding model input:\n",
    "* centroid position\n",
    "* morphology\n",
    "    * area\n",
    "    * perimeter\n",
    "    * eccentricity\n",
    "\n",
    "Edges assigned to cells within 64 pixels(41.6 µm) of each other.  \n",
    "Model is given neighborhood embeddings and centroid positions of cells in the previous seven frames [t<sub>n-7</sub>,t<sub>n</sub>] to compare with t<sub>n+1</sub>.  \n",
    "The temporal context of the previous seven frames is modeled using long short-term memory (LSTM) layers\n",
    "\n",
    "These feature vectors are ten fed into a tracking model that causally integrates temporal information and performs a pairwise comparison of each cell's feature vector  \n",
    "across frames to produce an effective probability score indicating wheter two cells are the same cell, are different cells, or have a parent-child relationship.  \n",
    "Lineage tracking and hungarian \"shadow object\" algorithm to assign birth or death of cells.\n",
    "\n",
    "#### Imaging details\n",
    "1:1 ratio of T-cells:cancer cells  \n",
    "Brightfield images capture T-cells & cancer cells  \n",
    "Red fluorescent protein channel images capture cancer nuclei  \n",
    "Images made every 4 minutes for 24 hours\n",
    "\n",
    "#### Methods \n",
    "##### Neighborhood encoder architecture in detail\n",
    "constructed a graph attention network <sup>[1]</sup> with 3 inputs  \n",
    "1. First head received images of each cell and converted these images to a vector embedding with a convolutional neural network.\n",
    "    * Each image consisted of a 16x16 crop of the raw data centered on the centroid position of the cell. \n",
    "    * Pixels within the nuclear segmentation mask were normalized by subtracting the mean value and dividing by the standard deviation.\n",
    "2. Second head received the centroid location of each cell\n",
    "3. Third head received three morphology metrics for each cell.\n",
    "    * area\n",
    "    * perimeter\n",
    "    * eccentricity\n",
    "\n",
    "The latter two heads made use of fully connected neural networks to convert the inputs into vectors  \n",
    "Adjacency matrix for the graph attention network based on the Euclidean distance between pairs of cells  \n",
    "Cells were linked if they were closer than 64 pixels (41.6 µm).  \n",
    "\n",
    "Normalized adjacency matrix and concatenated embeddings were fed into a graph attention layer to update the embeddings of each cell.\n",
    "The appearance and morphology embeddings were concatenated to the output of the graph attention layer to generate the final neighborhood embedding\n",
    "\n",
    "**References**  \n",
    "[1]: Brody, S., Alon, U., & Yahav, E. (2021). How attentive are graph attention networks?. arXiv preprint arXiv:2105.14491."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The markov matrix looks at cell density to see how the populations move across the whole field of view, while the neighborhood encoder looks at the invididual cell inside the bin to see where it is.\n",
    "For example there is a difference between 4 cancer cells and 1 T cell where the 4 cancer cells are clustered together and the T cell is close vs 4 cancer cells that surround the T cell but at equal distance from the 1 T cell\n",
    "And the neighborhood encoding captures this subtle difference the markov matrix can't capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
